# Лабораторная работа №2: Прогнозирование длительности поездок на такси в Нью-Йорке

## Обзор

Данная лабораторная работа посвящена применению методов машинного обучения для решения задачи **регрессии** — прогнозирования длительности поездок на такси в Нью-Йорке. В работе используется датасет с Kaggle "NYC Taxi Trip Duration".

**Цель работы:** построить модель машинного обучения, способную предсказывать длительность поездки на такси на основе различных признаков (местоположение посадки/высадки, время, количество пассажиров и т.д.).

**Метрика оценки:** RMSLE (Root Mean Squared Logarithmic Error) — корень из среднеквадратичной логарифмической ошибки. Эта метрика выбрана, поскольку она менее чувствительна к выбросам и хорошо работает с асимметричными распределениями, характерными для данных о длительности поездок.

---

## Структура лабораторной работы

Работа разделена на несколько частей, каждая из которых последовательно улучшает модель:

1. **Часть 0:** Подготовка данных (1 задание)
2. **Часть 1:** Временные признаки (4 задания, 8 баллов)
3. **Часть 2:** Географические признаки (5 заданий)
4. **Часть 3:** Остальные признаки (2 задания)
5. **Часть 4:** Оптимизация модели (3 задания, 6 баллов)
6. **Бонусные задания:** Альтернативные системы координат и продвинутые геопространственные признаки

**Общая оценка:** ~18+ баллов

---

## Часть 0: Подготовка данных (Задание 1)

### Описание

В этой части выполняется загрузка и первичная обработка данных:

1. Загрузка датасета из CSV-файла
2. Удаление столбца `dropoff_datetime` (время окончания поездки)
3. Создание целевой переменной из `trip_duration`

### Код

```python
df = pd.read_csv("data/input/laboratory_work2/train.csv")
df = df.drop('dropoff_datetime', axis=1)
```

### Анализ распределения целевой переменной

Длительность поездок имеет **сильно правостороннее распределение** — большинство поездок короткие, но есть редкие очень длинные поездки.

### Логарифмическое преобразование

Для улучшения работы модели применяется логарифмическое преобразование:

```python
df['log_trip_duration'] = np.log1p(df['trip_duration'])
```

Функция `np.log1p(x)` вычисляет `log(x + 1)`, что предотвращает проблемы с нулевыми значениями.

### Метрика RMSLE

Функция для расчета метрики:

```python
def rmsle(log1p_y_true, log1p_y_pred):
    """
    Вычисляет RMSLE (Root Mean Squared Logarithmic Error)
    Принимает уже логарифмированные значения
    """
    return np.sqrt(np.mean((log1p_y_true - log1p_y_pred) ** 2))
```

---

## Часть 1: Временные признаки (Задания 2-5)

### Задание 2: Извлечение временных признаков

**Цель:** Извлечь из даты и времени поездки полезные признаки.

Из столбца `pickup_datetime` извлекаются:
- `day_of_year` — день года (1-366)
- `day_of_week` — день недели (0=понедельник, 6=воскресенье)
- `month` — месяц (1-12)
- `hour` — час дня (0-23)

```python
df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])
df['day_of_year'] = df['pickup_datetime'].dt.dayofyear
df['day_of_week'] = df['pickup_datetime'].dt.dayofweek
df['month'] = df['pickup_datetime'].dt.month
df['hour'] = df['pickup_datetime'].dt.hour
```

### Анализ по дням

График количества поездок по дням выявляет **две крупные аномалии**:
1. **Снежный шторм Jonas** (около 23 января) — резкое падение количества поездок
2. **Новый год** (1 января) — также снижение активности

### Задание 3: Детальный анализ по часам и дням недели

**Паттерны активности:**
- **Часы пик:** 7-9 утра и 18-20 вечера (rush hours)
- **Ночные часы:** минимальная активность 2-5 часов ночи
- **Выходные дни:** пятница и суббота имеют иные вечерние паттерны (развлечения)

Визуализация с помощью `seaborn.lineplot` показывает различия в количестве поездок по часам для разных дней недели.

### Задание 4: Анализ длительности по временным признакам

**Интересная закономерность:** существует **обратная зависимость** между количеством поездок и их длительностью:
- **Ночные часы** (низкое количество поездок) → **большая длительность**
- **Часы пик** (высокое количество поездок) → **меньшая длительность**

Это объясняется тем, что ночью меньше пробок, но поездки в среднем длиннее, а в часы пик много коротких городских поездок.

### Задание 5: Первая модель с временными признаками

Построение модели Ridge-регрессии только на временных признаках:

```python
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Категориальные признаки кодируются через OneHotEncoder
categorical_features = ['day_of_week', 'month', 'hour']
# Числовые признаки нормализуются через StandardScaler
numerical_features = ['day_of_year']

model = Pipeline([
    ('preprocessor', ColumnTransformer([
        ('cat', OneHotEncoder(sparse=False), categorical_features),
        ('num', StandardScaler(), numerical_features)
    ])),
    ('regressor', Ridge(alpha=1.0))
])

model.fit(X_train, y_train)
```

**Ridge-регрессия** — это линейная регрессия с L2-регуляризацией, которая предотвращает переобучение путем штрафования больших весов.

---

## Часть 2: Географические признаки (Задания 6-10)

### Задание 6: Расстояние по формуле Хаверсина

**Формула Хаверсина** вычисляет расстояние между двумя точками на сфере (великая окружность):

```python
def haversine_distance(lat1, lon1, lat2, lon2):
    """
    Вычисляет расстояние между двумя точками на Земле
    в километрах, используя формулу Хаверсина
    """
    R = 6371.0  # Радиус Земли в километрах

    lat1_rad = np.radians(lat1)
    lat2_rad = np.radians(lat2)
    delta_lat = np.radians(lat2 - lat1)
    delta_lon = np.radians(lon2 - lon1)

    a = np.sin(delta_lat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(delta_lon / 2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

    return R * c
```

Создается признак `haversine` и его логарифмированная версия `log_haversine`, которая имеет **лучшую корреляцию** с целевой переменной.

### Задание 7: Анализ скорости

Вычисляется средняя скорость поездки:

```python
speed = haversine_distance / trip_duration
```

**Важно:** Признак скорости **нельзя использовать** для обучения модели, так как он требует знания `trip_duration` (целевой переменной). Однако анализ скорости полезен для понимания данных:
- **Минимальная скорость** — в часы пик (пробки)
- **Максимальная скорость** — ночью (свободные дороги)

### Задание 8: Важные локации

Создаются признаки на основе расстояния до ключевых точек Нью-Йорка:

1. **JFK Airport** (аэропорт Кеннеди): 40.6413°N, 73.7781°W
2. **LaGuardia Airport** (аэропорт Ла-Гуардия): 40.7769°N, 73.8740°W
3. **Penn Station** (вокзал): 40.7505°N, 73.9934°W

Для каждой локации создаются 2 признака:
- Расстояние от точки **посадки** до локации
- Расстояние от точки **высадки** до локации

```python
def is_near_location(lat, lon, target_lat, target_lon, threshold_km=0.5):
    """
    Проверяет, находится ли точка в пределах threshold_km от целевой локации
    """
    distance = haversine_distance(lat, lon, target_lat, target_lon)
    return distance <= threshold_km
```

**Результат анализа:** поездки в/из аэропортов имеют значительно большую длительность.

### Задание 9: MapGridTransformer — сетка на карте

Создается кастомный трансформер для scikit-learn, который разбивает географическую область на сетку:

```python
from sklearn.base import BaseEstimator, TransformerMixin

class MapGridTransformer(BaseEstimator, TransformerMixin):
    """
    Разбивает карту на сетку n_rows × n_cols ячеек
    Создает категориальные признаки pickup_cell и dropoff_cell
    """
    def __init__(self, n_rows=20, n_cols=20):
        self.n_rows = n_rows
        self.n_cols = n_cols

    def fit(self, X, y=None):
        # Определяет границы области на основе обучающих данных
        self.lat_min = X[:, 0].min()
        self.lat_max = X[:, 0].max()
        self.lon_min = X[:, 1].min()
        self.lon_max = X[:, 1].max()
        return self

    def transform(self, X):
        # Преобразует координаты в номера ячеек
        pickup_cells = self._get_cell_id(X[:, 0], X[:, 1])
        dropoff_cells = self._get_cell_id(X[:, 2], X[:, 3])
        return np.column_stack([pickup_cells, dropoff_cells])
```

**Идея:** разные части города имеют разные паттерны поездок. Разбивка на сетку 20×20 позволяет модели учесть локальные особенности.

**Преимущества:**
- Совместим с `sklearn.pipeline.Pipeline`
- Автоматически адаптируется к границам данных через метод `fit()`
- Создает дискретные признаки, которые могут быть закодированы через OneHotEncoder

### Задание 10: Модель с географическими признаками

Строится модель с добавлением координат и признаков расстояний:

```python
numerical_features = ['day_of_year', 'log_haversine',
                     'pickup_jfk_distance', 'dropoff_jfk_distance',
                     'pickup_lga_distance', 'dropoff_lga_distance',
                     'pickup_penn_distance', 'dropoff_penn_distance']
categorical_features = ['day_of_week', 'month', 'hour',
                       'pickup_cell', 'dropoff_cell']
```

---

## Часть 3: Остальные признаки (Задания 11-12)

### Задание 11: Анализ категориальных признаков

Анализируются оставшиеся признаки в датасете:

1. **vendor_id** — идентификатор компании-перевозчика (1 или 2)
2. **passenger_count** — количество пассажиров (1-9)
3. **store_and_fwd_flag** — флаг хранения данных (N/Y)

Хотя их влияние на длительность поездки небольшое, они все равно добавляются в модель для полноты.

### Задание 12: Модель со всеми признаками

Финальная модель объединяет:
- Временные признаки
- Географические признаки
- Категориальные признаки

```python
model = Pipeline([
    ('preprocessor', ColumnTransformer([
        ('cat', OneHotEncoder(sparse=False, handle_unknown='ignore'),
         categorical_features),
        ('num', StandardScaler(), numerical_features)
    ])),
    ('regressor', Ridge(alpha=1.0))
])
```

---

## Часть 4: Оптимизация модели (Задания 13-15)

### Задание 13: Удаление выбросов

**Проблема:** датасет содержит аномальные значения, которые ухудшают качество модели.

**Решение:** удаление выбросов на основе квантилей:

```python
# Определение границ для haversine
haversine_low = train_df['haversine'].quantile(0.01)
haversine_high = train_df['haversine'].quantile(0.99)

# Определение границ для длительности
duration_low = train_df['log_trip_duration'].quantile(0.01)
duration_high = train_df['log_trip_duration'].quantile(0.99)

# Фильтрация данных
train_df_clean = train_df[
    (train_df['haversine'] >= haversine_low) &
    (train_df['haversine'] <= haversine_high) &
    (train_df['log_trip_duration'] >= duration_low) &
    (train_df['log_trip_duration'] <= duration_high)
]
```

**Объединение редких категорий:**

Ячейки сетки с малым количеством наблюдений (< 100) объединяются в категорию "rare":

```python
cell_counts = train_df['pickup_cell'].value_counts()
rare_cells = cell_counts[cell_counts < 100].index
train_df['pickup_cell'] = train_df['pickup_cell'].replace(rare_cells, 'rare')
```

### Задание 14: Сравнение Ridge и Lasso регрессии

**Ridge регрессия** (L2-регуляризация):
- Штрафует квадрат весов: `penalty = α × Σ(w²)`
- Уменьшает веса, но редко обнуляет их полностью

**Lasso регрессия** (L1-регуляризация):
- Штрафует модуль весов: `penalty = α × Σ(|w|)`
- Может обнулять веса → автоматический отбор признаков

**Подбор гиперпараметра α:**

```python
alphas = np.logspace(-3, 4, 50)  # От 0.001 до 10000

ridge_scores = []
lasso_scores = []

for alpha in alphas:
    ridge_model = Ridge(alpha=alpha)
    ridge_model.fit(X_train, y_train)
    ridge_pred = ridge_model.predict(X_val)
    ridge_scores.append(mean_squared_error(y_val, ridge_pred))

    lasso_model = Lasso(alpha=alpha)
    lasso_model.fit(X_train, y_train)
    lasso_pred = lasso_model.predict(X_val)
    lasso_scores.append(mean_squared_error(y_val, lasso_pred))
```

Строится график зависимости MSE от α, выбирается модель с наименьшей ошибкой на валидационной выборке.

### Задание 15: Признаки взаимодействия (route)

**Идея:** комбинация ячейки посадки и ячейки высадки создает уникальный **маршрут**, который может иметь специфические характеристики.

```python
train_df['route'] = train_df['pickup_cell'] + "_" + train_df['dropoff_cell']
```

**Пример:** `"cell_123_cell_456"` — маршрут из ячейки 123 в ячейку 456.

**Почему это важно:** линейная модель не может автоматически выявить взаимодействие между признаками. Явное создание признака `route` позволяет модели учесть, что, например, поездка из центра в аэропорт всегда долгая, независимо от часа дня.

---

## Бонусная часть 1: Альтернативные системы координат (Задание 16)

### Манхэттенское расстояние с поворотом координат

**Проблема:** улицы Манхэттена расположены под углом к направлениям север-юг.

**Решение:** поворот системы координат на 45°:

```python
def rotate_coordinates(lat, lon, angle_degrees):
    """
    Поворачивает координаты на указанный угол
    для выравнивания с уличной сеткой
    """
    angle_rad = np.radians(angle_degrees)
    x = lat * np.cos(angle_rad) - lon * np.sin(angle_rad)
    y = lat * np.sin(angle_rad) + lon * np.cos(angle_rad)
    return x, y

manhattan_x, manhattan_y = rotate_coordinates(
    df['pickup_latitude'],
    df['pickup_longitude'],
    45
)
```

**Манхэттенское расстояние:**

```python
manhattan_distance = abs(pickup_x - dropoff_x) + abs(pickup_y - dropoff_y)
```

Это расстояние лучше отражает реальные маршруты такси в городе с прямоугольной сеткой улиц.

---

## Бонусная часть 2: H3 гексагональная индексация (Задание 17)

### Что такое H3?

**H3** — это система иерархической гексагональной индексации, разработанная Uber. Она разбивает Землю на шестиугольные ячейки разного размера (разрешение от 0 до 15).

**Преимущества шестиугольников:**
- Равное расстояние до всех соседних ячеек
- Более естественное представление геопространственных данных
- Иерархическая структура (каждая ячейка содержит 7 дочерних)

### H3Transformer

```python
import h3

class H3Transformer(BaseEstimator, TransformerMixin):
    """
    Создает признаки на основе H3 гексагональной индексации
    """
    def __init__(self, resolution=7):
        self.resolution = resolution

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        pickup_h3 = []
        dropoff_h3 = []

        for i in range(len(X)):
            pickup_lat, pickup_lon = X[i, 0], X[i, 1]
            dropoff_lat, dropoff_lon = X[i, 2], X[i, 3]

            pickup_h3.append(
                h3.geo_to_h3(pickup_lat, pickup_lon, self.resolution)
            )
            dropoff_h3.append(
                h3.geo_to_h3(dropoff_lat, dropoff_lon, self.resolution)
            )

        return np.column_stack([pickup_h3, dropoff_h3])
```

**Параметр resolution:**
- resolution=7: ячейки ~5 км²
- resolution=8: ячейки ~0.7 км²
- resolution=9: ячейки ~0.1 км²

Более высокое разрешение = более детальная сегментация, но больше категорий для кодирования.

---

## Используемые библиотеки

### Обработка данных
- **pandas** — работа с табличными данными (DataFrames)
- **numpy** — численные вычисления, массивы

### Визуализация
- **matplotlib** — построение графиков (линии, точки, гистограммы)
- **seaborn** — статистическая визуализация (lineplot, boxplot)
- **folium** — интерактивные карты с маркерами

### Машинное обучение (scikit-learn)
- **model_selection.train_test_split** — разбиение на обучающую и тестовую выборки
- **linear_model.Ridge, Lasso** — модели регрессии с регуляризацией
- **preprocessing.StandardScaler** — нормализация числовых признаков
- **preprocessing.OneHotEncoder** — кодирование категориальных признаков
- **compose.ColumnTransformer** — применение разных преобразований к разным столбцам
- **pipeline.Pipeline** — объединение предобработки и модели
- **base.BaseEstimator, TransformerMixin** — базовые классы для создания кастомных трансформеров
- **metrics.mean_squared_error** — метрика для оценки

### Геопространственные вычисления
- **h3** — иерархическая гексагональная индексация

---

## Последовательность построения моделей

```
Модель 1: Только временные признаки
         ↓ (добавляем координаты)
Модель 2: Временные + географические
         ↓ (добавляем остальные признаки)
Модель 3: Все базовые признаки
         ↓ (очистка от выбросов)
Модель 4: Очищенные данные
         ↓ (подбор регуляризации)
Модель 5: Оптимизированная Ridge/Lasso
         ↓ (добавляем взаимодействия)
Модель 6: С признаком route
         ↓ (альтернативные признаки)
Модель 7: Манхэттенское расстояние
Модель 8: H3 гексагональная сетка
```

Каждая следующая модель строится на основе предыдущей, постепенно улучшая качество предсказаний.

---

## Ключевые концепции машинного обучения

### 1. Feature Engineering (Конструирование признаков)
Создание новых признаков из существующих данных:
- Извлечение компонентов времени (час, день недели)
- Вычисление расстояний
- Разбиение пространства на ячейки
- Создание признаков взаимодействия

### 2. Pipeline (Конвейер обработки)
Последовательное применение преобразований и модели:
```
Сырые данные → Предобработка → Модель → Предсказания
```

### 3. Регуляризация
Предотвращение переобучения путем штрафования сложных моделей:
- **L2 (Ridge):** все признаки, малые веса
- **L1 (Lasso):** отбор признаков, обнуление весов

### 4. Кросс-валидация
Разбиение данных на обучающую и валидационную выборки для объективной оценки качества модели.

### 5. Обработка выбросов
Удаление аномальных значений для повышения устойчивости модели.

### 6. Категориальное кодирование
OneHotEncoder преобразует категории в бинарные признаки:
```
vendor_id=1 → [1, 0]
vendor_id=2 → [0, 1]
```

---

## Рекомендации по выполнению

1. **Последовательность:** Выполняйте задания по порядку, каждое следующее строится на предыдущем
2. **Анализ данных:** Всегда визуализируйте данные перед построением модели
3. **Проверка на тестовой выборке:** Оценивайте модель на данных, которые не участвовали в обучении
4. **Эксперименты:** Пробуйте разные значения гиперпараметров (α для Ridge/Lasso, размер сетки)
5. **Интерпретация:** Понимайте, почему тот или иной признак улучшает модель

---

## Файлы проекта

- **laboratory_work2.ipynb** — основной notebook с кодом и результатами
- **laboratory_work2.md** — данный файл с пояснениями
- **data/input/laboratory_work2/train.csv** — обучающие данные
- **data/input/laboratory_work2/test.csv** — тестовые данные (если есть)

---

## Полезные ссылки

- [Kaggle: NYC Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration)
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [H3 Geospatial Indexing](https://h3geo.org/)
- [Haversine Formula](https://en.wikipedia.org/wiki/Haversine_formula)

---

**Автор:** Лабораторная работа №2 по машинному обучению
**Дата:** 2025
**Файл:** `laboratory_work2.ipynb`
